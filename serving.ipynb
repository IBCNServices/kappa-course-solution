{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip install influxdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "\n",
    "client = InfluxDBClient('10.10.139.45', 8086, 'root', 'root', 'example')\n",
    "client.create_database('example')\n",
    "\n",
    "def process(row):\n",
    "    client.write_points([{\n",
    "            \"measurement\": \"clicks\",\n",
    "            \"tags\": {\n",
    "                \"visitor_browser\": row['visitor_browser'],\n",
    "                \"visitor_country\": row['visitor_country'],\n",
    "            },\n",
    "            \"time\": row['ts_ingest'],\n",
    "            \"fields\": {\n",
    "                \"visitor_platform\": row['visitor_platform'],\n",
    "                \"article_title\": row['article_title'],\n",
    "                \"article\": row['article'],\n",
    "                \"visitor_os\": row['visitor_os'],\n",
    "                \"visitor_page_timer\": row['visitor_page_timer'],\n",
    "                \"visitor_page_height\": row['visitor_page_height'],\n",
    "            },\n",
    "        }], time_precision='ms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\",\"10.10.139.63:9092\") \\\n",
    "    .option(\"subscribe\", \"testing\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"visitor_platform\", StringType()),\n",
    "    StructField(\"ts_ingest\", LongType()),\n",
    "    StructField(\"article_title\", StringType()),\n",
    "    StructField(\"visitor_country\", StringType()),\n",
    "    StructField(\"visitor_os\", StringType()),\n",
    "    StructField(\"article\", StringType()),\n",
    "    StructField(\"visitor_browser\", StringType()),\n",
    "    StructField(\"visitor_page_timer\", IntegerType()),\n",
    "    StructField(\"visitor_page_height\", IntegerType()),\n",
    "])\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "dfs = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "      .select(from_json(col(\"value\"), schema) \\\n",
    "      .alias(\"clicks\"))\n",
    "\n",
    "df_data = dfs.select(\"clicks.*\")\n",
    "\n",
    "query = df_data.writeStream.foreach(process).start()\n",
    "query.awaitTermination()\n",
    "\n",
    "# Debug in terminal\n",
    "# Docs output modes https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\n",
    "# query = df_data_grouped.writeStream.outputMode(\"update\").option(\"truncate\", \"false\").format(\"console\").start()\n",
    "# query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
