{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa Architecture hands-on lab\n",
    "\n",
    "During the previous courses, you've been introduced to a number of tools and techniques to store and process data. This hands-on lab explains how you can combine these tools and techniques to get an end-to-end data processing pipeline.\n",
    "\n",
    "## Use case: a Flemish website\n",
    "\n",
    "Throughout this lab, you will build a platform to capture and analyze traffic to a website. You will build the following components:\n",
    "\n",
    "* An HTTP endpoint to gather pageview data from a website\n",
    "* A data cleaning pipeline for the pageview data\n",
    "* A security alerting system that detects DDoS attacks and requests trying to access your management platform.\n",
    "* A real-time dashboard showing the number of users on your website at any time.\n",
    "\n",
    "You'll also learn how to load the pageview data into a time-series database so it's accessible for Business Intelligence tools.\n",
    "\n",
    "The dataset used in this lab is a three-hour window of page view data of a popular Flemish news website. The dataset is anonymized and the URL's are changed to point to Wikipedia pages in order to protect the privacy of the users and the website owner. Although the dataset of this three hour window is only 500 MB in size, the original dataset's compressed size is ~1 TB. We used a similar setup to the one explained in the lab to analyze and predict the popularity of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Kappa Architecture\n",
    "\n",
    "[The Kappa Architecture](http://milinda.pathirage.org/kappa-architecture.com/) is Big Data processing design pattern. It is a high-level description of how to combine data analytics tools to solve real problems. This isn't a one size fits all solution. Many other data processing design patterns exist such as [the Lambda architecture](http://lambda-architecture.net/) and [the Zeta architecture](https://mapr.com/solutions/zeta-enterprise-architecture/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Kappa architecture focuses around \"events\" and uses an event log as the source of truth. So instead of storing the final state of your data you store each individual event, so for the platform of this lab we'll store each individual page load and use that event log as our source of truth. We then calculate the visitors per minute from that event log and store the result in a *serving database*. Apps and tools who need to know the number of visitors connect to the serving database to get their information, just like they would in a traditional data processing architecture. The actual raw page view data, however, is still available for reprocessing. The Kappa architecture is ideal for time-series data such as page views, logs or IoT sensors. \n",
    "\n",
    "\n",
    "Below you see the Kappa architecture applied to our website pageviews example.\n",
    "\n",
    "1. The website contains javascript code that sends an HTTP POST request to the ingest API when the user first loads the website and when the user scrolls down.\n",
    "1. The ingest API publishes each individual event on the \"clicks\" topic on Kafka, a distributed queue.\n",
    "1. Spark Streaming jobs subscribe to the \"clicks\" topic, process the clicks and write the processed data to serving databases and new Kafka topics.\n",
    "1. Applications such as the Tableau BI platform or Grafana connect to a serving database and use the data.\n",
    "\n",
    "<img src=\"img/Big Data Hands On - Kappa.png\">\n",
    "\n",
    "This has enormous flexibility.\n",
    "\n",
    "* If you update the stream processing code that generates the databases, you can easily replay all the events in order to create a new version of the serving layer. This can happen in parallel to the existing serving layer.\n",
    "* It's easy to have multiple different types of databases in the serving layer, each optimized for a specific application but built from the same dataset.\n",
    "* It's easily horizontally scalable but can still integrate with non-scalable applications.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Below is the architecture of the Kappa-inspired platform you'll build in this lab. Specifically, you'll create the following components, each in their own notebook. Start with the first component and come back to this notebook to continue with the next component when the first component is finished.\n",
    "\n",
    "1. [Create an ingest API using the Python Flask framework](endpoint.ipynb).\n",
    "2. [Clean the data using Spark Structured Streaming](cleanup.ipynb).\n",
    "3. [Generate security alerts](security.ipynb) and [generate a notification when such an alert happens.](security-notifications.ipynb)\n",
    "4. [Transform the data and load it into InfluxDB](serving.ipynb)\n",
    "5. [Create a realtime dashboard using Spark](dashboard-generation.ipynb).\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "When working on UGain desktops (or laptops with similar specs) you have to be careful with the resources available. Running the full lab at the same time is **not** possible. For each part of the lab we describe which notebooks should be running. If the notebook is not listed, it is expected to be shutdown.\n",
    "\n",
    "- Part 1 (Ingest API):\n",
    "\t-  [endpoint.ipynb](endpoint.ipynb)\n",
    "\t-  [fake-website.ipynb](fake-website.ipynb)\n",
    "\t-  [debug.ipynb](debug.ipynb)\n",
    "- Part 2 (Data cleanup):\n",
    "\t-  [endpoint.ipynb](endpoint.ipynb)\n",
    "\t-  [fake-website.ipynb](fake-website.ipynb)\n",
    "\t-  [debug.ipynb](debug.ipynb)\n",
    "\t-  [cleanup.ipynb](cleanup.ipynb)\n",
    "- Part 3 (Security):\n",
    "\t-  [endpoint.ipynb](endpoint.ipynb)\n",
    "\t-  [fake-website.ipynb](fake-website.ipynb)\n",
    "\t-  [debug.ipynb](debug.ipynb)\n",
    "\t-  [cleanup.ipynb](cleanup.ipynb)\n",
    "\t-  [security.ipynb](security.ipynb)\n",
    "\t-  [security-notifications.ipynb](security-notifications.ipynb)\n",
    "\t-  [fake-ddos.ipynb](fake-ddos.ipynb) (**if fake-intrusion is not running**)\n",
    "\t-  [fake-intrusion.ipynb](fake-intrusion.ipynb)(**if fake-ddos is not running**)\n",
    "- Part 4 (serving):\n",
    "\t-  [endpoint.ipynb](endpoint.ipynb)\n",
    "\t-  [fake-website.ipynb](fake-website.ipynb)\n",
    "\t-  [debug.ipynb](debug.ipynb)\n",
    "\t-  [cleanup.ipynb](cleanup.ipynb)\n",
    "\t-  [serving.ipynb](serving.ipynb)\n",
    "\n",
    "**Always save** the notebook **before** running cells. If there is a resource problem the browser might have to be restarted and code might be lost.\n",
    "\n",
    "\n",
    "<img src=\"img/use-case-overview.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Code Completion in Jupyter\n",
    "\n",
    "Jupyter has code completion abilities, but it doesn't show suggestions by default.\n",
    "\n",
    "**Show suggestions by pressing `tab` after a dot.**\n",
    "\n",
    "![image.png](img/suggestions.png)\n",
    "\n",
    "**Show function arguments by pressing `shift` - `tab` after the opening bracket of a function.**\n",
    "\n",
    "![image.png](img/function-docs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Resetting the environment\n",
    "\n",
    "All data in Kafka and InfluxDB is stored inside of their docker containers. Spark runs inside of the notebook container. You can remove this data and start from a fresh instance by stopping and removing their docker containers.\n",
    "\n",
    "```bash\n",
    "docker stop kafka; docker rm kafka\n",
    "docker stop influxdb; docker rm influxdb\n",
    "docker stop notebook; docker rm notebook\n",
    "```\n",
    "\n",
    "If there are any checkpoint directories remove those as well. Checkpoint directories are created after running kafka sink queries and store the state of which messages to process next.\n",
    "\n",
    "```bash\n",
    "rm -rf checkpoints*\n",
    "```\n",
    "\n",
    "After these operations you can create new containers.\n",
    "\n",
    "```bash\n",
    "# Start Kafka\n",
    "docker start kafka || docker run -d -p 2181:2181 -p 9092:9092 --env ADVERTISED_HOST=0.0.0.0 --env ADVERTISED_PORT=9092 --name kafka spotify/kafka\n",
    "\n",
    "# Start InfluxDB\n",
    "docker start influxdb || docker run -d -p 8086:8086 --name influxdb influxdb\n",
    "cd ~/Desktop/kappa-course\n",
    "\n",
    "# Start the notebook (with Spark)\n",
    "docker start -i notebook || docker run -it --net=\"host\" -v \"$PWD\":/home/jovyan/work --name notebook jupyter/pyspark-notebook\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
