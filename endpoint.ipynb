{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting pageview data\n",
    "\n",
    "In this exercise, you create an HTTP endpoint that ingests page view data (clicks) into the platform. Every time a user clicks on a link and when the user scrolls to certain positions on the page, a json \"click\" objects gets sent to this endpoint.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"visitor_platform\": \"mobile\",\n",
    "    #\n",
    "    # Timestamp of the event (milliseconds since unix epoch)\n",
    "    \"ts_ingest\": 1515819844345,\n",
    "    \n",
    "    \"article_title\": \"Cercanías San Sebastián\",\n",
    "    \"visitor_country\": \"BE\",\n",
    "    \n",
    "    # Seconds the page was open before this event was sent.\n",
    "    # (0 when this event is sent immediately after the page was opened.)\n",
    "    \"visitor_page_timer\": 0,\n",
    "\n",
    "    \"visitor_os\": \"ios\",\n",
    "    \"article\": \"https://en.wikipedia.org/wiki/Cercan%C3%ADas_San_Sebasti%C3%A1n\",\n",
    "    \n",
    "    # How much the user scrolled before this event was sent.\n",
    "    # (0 when this event is sent while the user hasn't scrolled yet.)\n",
    "    \"visitor_page_height\": 0,\n",
    "    \n",
    "    \"visitor_browser\": \"unknown\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "We use the [Python Flask framework](http://flask.pocoo.org/) to create the ingest HTTP endpoint. Flask is a lightweight and simple, but very powerful framework to write HTTP webservers in Python. Flask powers the api's of many large web services such as [Netflix](https://medium.com/netflix-techblog/python-at-netflix-bba45dae649e), [Airbnb](https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8), [Uber](https://github.com/uber/clay) and [Reddit](https://stackshare.io/reddit/reddit).\n",
    "\n",
    "The clicks that our API recieves are stored in [Apache Kafka](https://kafka.apache.org/), a distributed streaming platform initially created by LinkedIn. Kafka stores large distributed queues, called *topics* and allows *producers* to send data to the queue and *consumers* to read data from the queue, all in a fault-tolerant and durable way.\n",
    "\n",
    "The sole responsibility of the Ingest API is to recieve click events from HTTP POST requests and put them on the `clicks` topic in Kafka. The Ingest API itself doesn't do any cleaning or filtering, this happens later in the pipeline. Using Kafka here has a number of advantages.\n",
    "\n",
    "* Kafka acts as a **buffer** between the ingest of events and the processing events. Downstream issues, such as the processing code crashing, don't affect the ingest of events. This also allows the platform to **gracefully handle spikes in load**. When the processing code can't handle the load, Kafka will gather a backlog of unprocessed events, but the events will not be lost and the processing code can catch up when the load decreases again.\n",
    "* Kafka allows multiple consumers to subscribe to the same topic. This makes it easy to have **multiple paralell processing pipelines** which recieve the same event stream. You can use this to run multiple versions of your code next to each other in order to do tests or quality assurance. Or to have a staging environment that recieves the same event stream as the production environment.\n",
    "* Kafka is a **language-agnostic** platform with many client libraries so you can use it to create heterogenous streaming analytics pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install the required Python 3 dependencies\n",
    "python3 -m pip install kafka-python flask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask introduction\n",
    "\n",
    "The following code snippet is everything you need to get a working \"Hello World\" HTTP api. You start the server by running the cell. (select the cell with the python code and press `ctrl`-`enter`).\n",
    "\n",
    "You can test the server using the `curl` command in a terminal.\n",
    "\n",
    "```txt\n",
    "$ curl http://localhost:5000\n",
    "Index Page\n",
    "$ curl http://localhost:5000/hello\n",
    "Hello, World\n",
    "```\n",
    "\n",
    "You stop the server by clicking on the \"stop\" button on the toolbar of this notebook.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Index Page\\n'\n",
    "\n",
    "@app.route('/hello')\n",
    "def hello():\n",
    "    return 'Hello, World\\n'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functionality\n",
    "\n",
    "* The ` @app.route(\"/\")` annotation is used to define the URL that executes a function. In the previous example, any call to `localhost:5000/` will run the `index` function.\n",
    "* You can use the variable `request` inside of such a function to get information about the request. For example, `request.json` is a dictionary generated from the `json` body of the request.\n",
    "\n",
    "Take a look at the [Flask docs](http://flask.pocoo.org/docs/1.0/quickstart/#routing) for more information.\n",
    "\n",
    "\n",
    "# Kafka Python\n",
    "\n",
    "Below is an example for how to publish data to kafka using Python.\n",
    "\n",
    "Take a look at the [`python-kafka` docs ](https://kafka-python.readthedocs.io/en/master/usage.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "topic =\"test-topic\"\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "# Note: The data you send must be binary\n",
    "producer.send(topic, b\"Hello World!\").get(timeout=30)\n",
    "\n",
    "# To consume latest messages and auto-commit offsets\n",
    "consumer = KafkaConsumer(\n",
    "    group_id='test-group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    consumer_timeout_ms=500) # Stop iteration if no message after 5sec\n",
    "tp = TopicPartition(topic,0)\n",
    "consumer.assign([tp])\n",
    "\n",
    "consumer.seek(tp, 0)         # Go to the beginning of the queue\n",
    "\n",
    "\n",
    "for message in consumer:\n",
    "    # message value and key are raw bytes -- decode if necessary!\n",
    "    # e.g., for unicode: `message.value.decode('utf-8')`\n",
    "    print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition,\n",
    "                                          message.offset, message.key,\n",
    "                                          message.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoint API\n",
    "\n",
    "> **Task:** Write a server that listens for `POST` requests to the url `/clicks`, reads the body of the request as `json`, and sends sends that body to the `clicks` topic on Kafka.\n",
    "\n",
    "You can use the [fake-website](fake-website.ipynb) notebook to simulate three hours of pageview data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "import re\n",
    "import os\n",
    "from flask import Flask, request, Response, send_file, send_from_directory\n",
    "from kafka import KafkaProducer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "app = Flask(__name__, static_url_path='')\n",
    "\n",
    "class Producer(object):\n",
    "    def __init__(self):\n",
    "        self.producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                      value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "    def produce(self, topic, data):\n",
    "        self.producer.send(topic, data).get(timeout=30)\n",
    "\n",
    "\n",
    "producer = Producer()\n",
    "i = 0\n",
    "\n",
    "\n",
    "'''\n",
    "/clicks [POST]\n",
    "-----\n",
    "\n",
    "'''\n",
    "@app.route('/clicks', methods=['POST'])\n",
    "def extranm():\n",
    "    global i\n",
    "    if i > 1000:\n",
    "        clear_output()\n",
    "        i = 0\n",
    "    i = i+1\n",
    "    \n",
    "    rjson = request.json\n",
    "    \n",
    "    if not rjson:\n",
    "        return json.dumps({'success':False, 'message': 'could not decode json'}), 400, {'ContentType':'application/json'}\n",
    "\n",
    "    print(rjson)\n",
    "    producer.produce('clicks', rjson)\n",
    "    return json.dumps({'success':True}), 200, {'ContentType':'application/json'}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
