{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"clicks-cleaned\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"visitor_platform\", StringType()),\n",
    "    StructField(\"ts_ingest\", TimestampType()),\n",
    "    StructField(\"article_title\", StringType()),\n",
    "    StructField(\"visitor_country\", StringType()),\n",
    "    StructField(\"visitor_os\", StringType()),\n",
    "    StructField(\"article\", StringType()),\n",
    "    StructField(\"visitor_browser\", StringType()),\n",
    "    StructField(\"visitor_page_timer\", IntegerType()),\n",
    "    StructField(\"visitor_page_height\", IntegerType()),\n",
    "])\n",
    "\n",
    "dfs = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "      .select(from_json(col(\"value\"), schema) \\\n",
    "      .alias(\"clicks\"))\n",
    "\n",
    "df_data = dfs.select(\"clicks.*\")\n",
    "\n",
    "@udf\n",
    "def forbidden_clicks(click_url):\n",
    "    return click_url.endswith('/admin')\n",
    "\n",
    "# For every article url let's check if it is a forbidden url\n",
    "# We can not use the map() function here, dataframes do not support this anymore since version 2.0\n",
    "# Under the hood calling map() on a dataframe would transform it to an rdd which is not allowed in structured streaming\n",
    "# It means you can use only DataFrame or SQL, conversion to RDD (or DStream or local collections) are not supported.\n",
    "# For this we will use a User Defined Function (UDF) to execute some Pyhton code on a column.\n",
    "df_forbidden = df_data.select('article', forbidden_clicks('article').cast('boolean').alias('forbidden'))\n",
    "\n",
    "# DDOS\n",
    "# Window over last X minutes, count number of 'visitor_page_timer' and 'visitor_page_height' == 0\n",
    "@pandas_udf('boolean', PandasUDFType.SCALAR)\n",
    "def ddos_flagged(page_timer, page_height):\n",
    "    return (page_timer == 0) & (page_height == 0)\n",
    "\n",
    "df_ddos = df_data.select(\"*\", ddos_flagged('visitor_page_timer', 'visitor_page_height').alias('flagged'))\n",
    "\n",
    "df_ddos_window = df_ddos.groupBy(\n",
    "    window(df_ddos.ts_ingest, '30 seconds'),\n",
    "    df_ddos.flagged\n",
    ").count()\n",
    "\n",
    "# Debug dataframes in terminal\n",
    "# query_data = df_data.writeStream.outputMode(\"append\").option(\"truncate\", \"false\").format(\"console\").start()\n",
    "# query_forbidden = df_forbidden.writeStream.outputMode(\"append\").option(\"truncate\", \"false\").format(\"console\").start()\n",
    "# query_ddos = df_ddos_window.writeStream.outputMode(\"update\").option(\"truncate\", \"true\").format(\"console\").start()\n",
    "\n",
    "query_forbidden = df_forbidden.selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream.format(\"kafka\") \\\n",
    "    .outputMode('update') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"clicks-calculated-forbidden\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointsforbidden\") \\\n",
    "    .start()\n",
    "\n",
    "query_ddos = df_ddos_window.selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream.format(\"kafka\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .outputMode('update') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"clicks-calculated-ddos\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointsddos\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
