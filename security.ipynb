{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDoS and Intrusion detection\n",
    "\n",
    "\n",
    "In this exercise, we'll use Spark structured streaming to detect DDoS attacks and attempts to access the admin panel of the website.\n",
    "\n",
    "* Use the [fake-ddos](fake-ddos.ipynb) notebook to simulate a DDoS attack.\n",
    "* Use the [fake-intrusion](fake-intrusion.ipynb) notebook to simulate an intrusion attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure the required Python 3 dependencies are installed.\n",
    "python3 -m pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark context and specify that the python spark-kafka libraries need to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 pyspark-shell'\n",
    "\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a streaming DataFrame that represents the events received from the Kafka topic `clicks-cleaned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"clicks-cleaned\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the json to columns in the DataFrame. Make sure to use TimestampType for the `ts_ingest` since we already converted it in the `clean` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"visitor_platform\", StringType()),\n",
    "    StructField(\"ts_ingest\", TimestampType()),\n",
    "    StructField(\"article_title\", StringType()),\n",
    "    StructField(\"visitor_country\", StringType()),\n",
    "    StructField(\"visitor_os\", StringType()),\n",
    "    StructField(\"article\", StringType()),\n",
    "    StructField(\"visitor_browser\", StringType()),\n",
    "    StructField(\"visitor_page_timer\", IntegerType()),\n",
    "    StructField(\"visitor_page_height\", IntegerType()),\n",
    "])\n",
    "\n",
    "dfs = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "      .select(from_json(col(\"value\"), schema) \\\n",
    "      .alias(\"clicks\"))\n",
    "\n",
    "df_data = dfs.select(\"clicks.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [user-defined function (`udf`)](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) `forbidden_clicks` which takes a URL as input and returns `True` if the URL points to the admin part of the website (when it ends with `/admin`).\n",
    "\n",
    "As an example, the following code creates a UDF which squares each value of a column. It is used on the \"id\" column and the resulting column's name is changed to \"id_squared\".\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def squared_udf(s):\n",
    "  return s * s\n",
    "df = spark.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def forbidden_clicks(click_url):\n",
    "    return click_url.endswith('/admin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the UDF to create the dataframe `df_forbidden` which contains the collumn `forbidden` which specifies if the URL is an admin URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every article url check if it is a forbidden url.\n",
    "# We can not use the map() function here, dataframes do not support this anymore since version 2.0.\n",
    "# Under the hood calling map() on a dataframe would transform it to an RDD which is not allowed in structured streaming.\n",
    "# This means you can use only DataFrame or SQL. Conversion to RDD (or DStream or local collections) are not supported.\n",
    "# Because of this we will use a User Defined Function (UDF) to execute some Pyhton code on a column.\n",
    "df_forbidden = df_data.select('article', forbidden_clicks('article').cast('boolean').alias('forbidden'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for detecting ddos attacks. First we want to flag whether an individual event is suspicious, i.e. whether the page_timer and page_height are both `0`. However, this time we'll use a `pandas_udf`.\n",
    "\n",
    "[Regular Python UDF's have the disadvantage that they operate on one row at a time](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html), causing them to suffer from high serialization and invocation overhead. Pandas UDF's are built on top of Apache Arrow to support high-performant UDF's in Python.\n",
    "\n",
    "This is the squared_udf converted to a pandas udf.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def squared_pandas_udf(s):\n",
    "    return s * s\n",
    "\n",
    "df = spark.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n",
    "```\n",
    "\n",
    "The regular UDF version works one row at a time: the user-defined function takes a long `s` and returns the result of `s*s` as a long. In the Pandas version, the user-defined function takes a pandas.Series `s` and returns the result of `s*s` as a pandas.Series. Because `s*s` is vectorized on `pandas.Series`, the Pandas version is much faster than the row-at-a-time version.\n",
    "\n",
    "Note that there are two important requirements when using scalar pandas UDFs:\n",
    "\n",
    "* The input and output series must have the same size.\n",
    "* How a column is split into multiple pandas.Series is internal to Spark, and therefore the result of user-defined function must be independent of the splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window over last X seconds, count number of 'visitor_page_timer' and 'visitor_page_height' == 0\n",
    "@pandas_udf('boolean', PandasUDFType.SCALAR)\n",
    "def ddos_flagged(page_timer, page_height):\n",
    "    return (page_timer == 0) & (page_height == 0)\n",
    "# use ddos_flagged to create df_ddos, where all suspicious events are flagged.\n",
    "df_ddos = df_data.select(\"*\", ddos_flagged('visitor_page_timer', 'visitor_page_height').alias('flagged'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step in detecting a ddos attack is counting how many suspicious events happen within a certain timeframe. For this, well combine `groupBy` and a 30 seconds `window` based on the `ts_ingest` timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ddos_window = df_ddos.groupBy(\n",
    "    window(df_ddos.ts_ingest, '30 seconds'),\n",
    "    df_ddos.flagged\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run these queries and write the output to `clicks-calculated-forbidden` and `clicks-calculated-ddos`. Use a trigger with `processingTime = \"30 seconds\"` for the ddos query so that the next interval is only calculated 30 seconds after the first interval starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug dataframes in terminal\n",
    "# query_data = df_data.writeStream.outputMode(\"append\").option(\"truncate\", \"false\").format(\"console\").start()\n",
    "# query_forbidden = df_forbidden.writeStream.outputMode(\"append\").option(\"truncate\", \"false\").format(\"console\").start()\n",
    "# query_ddos = df_ddos_window.writeStream.outputMode(\"update\").option(\"truncate\", \"true\").format(\"console\").start()\n",
    "\n",
    "query_forbidden = df_forbidden.selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream.format(\"kafka\") \\\n",
    "    .outputMode('update') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"clicks-calculated-forbidden\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointsforbidden\") \\\n",
    "    .start()\n",
    "\n",
    "query_ddos = df_ddos_window.selectExpr(\"to_json(struct(*)) as value\") \\\n",
    "    .writeStream.format(\"kafka\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .outputMode('update') \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"clicks-calculated-ddos\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointsddos\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
